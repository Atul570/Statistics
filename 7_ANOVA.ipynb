{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e812222",
   "metadata": {},
   "source": [
    "<font color = \"red\" size =12>**ANOVA (Analysis of Varience)**</font>\n",
    "\n",
    "<font color = \"red\">**F-Distribution: Key Concepts & Applications**</font>\n",
    "\n",
    "The **F-distribution** is a **continuous probability distribution** commonly used in **statistical hypothesis testing**, especially in **ANOVA (Analysis of Variance)** and comparing **variances between samples**. Below is a detailed breakdown of its key properties and applications.  \n",
    "\n",
    "\n",
    "\n",
    "**1. Continuous Probability Distribution**  \n",
    "- The **F-distribution** is used in hypothesis testing to analyze **variance ratios** from sample data.\n",
    "- Most commonly applied in **ANOVA** and regression analysis.\n",
    "\n",
    "\n",
    "\n",
    "**2. Fisher-Snedecor Distribution**  \n",
    "- Named after **Ronald Fisher** and **George Snedecor**, two influential statisticians.\n",
    "- Often referred to as the **Fisher-Snedecor distribution** in statistical literature.\n",
    "\n",
    "\n",
    "\n",
    "**3. Degrees of Freedom (df)**  \n",
    "- Defined by **two parameters**:  \n",
    "  - \\( df_1 \\) → Degrees of freedom for the **numerator** (between-group variance).  \n",
    "  - \\( df_2 \\) → Degrees of freedom for the **denominator** (within-group variance).  \n",
    "- These **df values** determine the **shape** of the F-distribution.\n",
    "\n",
    "\n",
    "\n",
    "**4. Shape: Positively Skewed & Bounded**  \n",
    "- The F-distribution is **positively skewed**, meaning it has a long tail on the right.\n",
    "- **Lower bound = 0**, since variance **cannot** be negative.\n",
    "- The shape depends on the **degrees of freedom** (higher df leads to a more symmetrical curve).\n",
    "\n",
    "\n",
    "\n",
    "**5. Testing Equality of Variances**  \n",
    "- Used for testing if **two population variances are equal**.\n",
    "- Example: **Levene’s Test** and **Bartlett’s Test** use the F-distribution for variance equality.\n",
    "\n",
    "\n",
    "\n",
    "### **6. Comparing Statistical Models**  \n",
    "- Helps **compare the fit of statistical models** by checking whether including additional variables **significantly improves a model**.\n",
    "- Used in **ANOVA** to test if group means differ significantly.\n",
    "\n",
    "\n",
    "\n",
    "**7. F-Statistic Calculation**  \n",
    "- The **F-statistic** is computed as:\n",
    "\n",
    "  $$\n",
    "  F = \\frac{\\text{Variance Between Groups}}{\\text{Variance Within Groups}}\n",
    "  $$\n",
    "\n",
    "- A **large F-statistic** suggests a **significant difference between groups**.\n",
    "- Compared against **critical values** from the F-distribution table to determine significance.\n",
    "\n",
    "\n",
    "\n",
    "**8. Applications of the F-Distribution**  \n",
    "- Used in **many fields**, including:\n",
    "  - **Psychology & Education** → Evaluating experimental designs.\n",
    "  - **Economics** → Comparing different financial models.\n",
    "  - **Social & Natural Sciences** → Analyzing variance in real-world data.\n",
    "- Essential for **hypothesis testing, regression analysis, and variance analysis**.\n",
    "\n",
    "\n",
    "\n",
    "**Key Takeaway**  \n",
    "The **F-distribution** is **essential** in **ANOVA**, hypothesis testing, and model comparisons. By evaluating **variance ratios**, it helps **determine statistical significance**, ensuring data-driven decision-making in various research and business applications.\n",
    "\n",
    "The **F-distribution** is created using the ratio of two **independent Chi-Square distributions**, scaled by their degrees of freedom. The shape of the F-distribution depends on **two degrees of freedom (df₁, df₂)**—one for the numerator and one for the denominator.\n",
    "\n",
    "**How to Create the F-Distribution**\n",
    "1. **Generate Two Independent Chi-Square Distributed Variables**  \n",
    "   - Let $ X_1 $ and $ X_2 $ be **Chi-Square distributed** random variables with degrees of freedom **df₁** and **df₂**, respectively.\n",
    "\n",
    "2. **Divide Each Chi-Square Variable by Its Degrees of Freedom**  \n",
    "   - Compute:  \n",
    "     $$\n",
    "     \\frac{X_1}{df_1} \\quad \\text{and} \\quad \\frac{X_2}{df_2}\n",
    "     $$\n",
    "   - This scales each value relative to its degrees of freedom.\n",
    "\n",
    "3. **Compute the Ratio to Obtain the F-Statistic**  \n",
    "   - The F-distribution follows:  \n",
    "     $$\n",
    "     F = \\frac{\\left( \\frac{X_1}{df_1} \\right)}{\\left( \\frac{X_2}{df_2} \\right)}\n",
    "     $$\n",
    "   - This gives the **F-statistic**, which follows an F-distribution.\n",
    "\n",
    "**Shape of the F-Distribution**\n",
    "- **Positively Skewed (Right-Skewed):**  \n",
    "  - The distribution has a long tail to the **right**, meaning most values are near zero, but extreme values can be large.\n",
    "  \n",
    "- **Lower Bound at 0:**  \n",
    "  - Since variance cannot be negative, F-values **cannot be less than 0**.\n",
    "\n",
    "- **Shape Varies by Degrees of Freedom:**  \n",
    "  - **Higher df₁ & df₂ → more symmetric, approaching normal distribution**.\n",
    "  - **Lower df₁ & df₂ → heavily skewed right**.\n",
    "\n",
    "![alt text](images\\F-distribution_pdf.svg.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9baf8b",
   "metadata": {},
   "source": [
    "<font color = \"red\">**One-Way ANOVA**</font>\n",
    "\n",
    "One-Way ANOVA (Analysis of Variance) is a statistical method used to compare the means of three or more independent groups to determine if there are any significant differences between them. It extends the t-test, which is used for comparing two groups, and helps analyze the effect of one independent variable with multiple levels.\n",
    "\n",
    "**Key Steps in One-Way ANOVA**\n",
    "1. **Define Hypotheses:**\n",
    "   - **Null Hypothesis (H₀):** All group means are equal.\n",
    "   - **Alternative Hypothesis (H₁):** At least one group mean is significantly different.\n",
    "\n",
    "2. **Calculate Means:**\n",
    "   - Compute the overall mean (**grand mean**) of all groups combined.\n",
    "   - Compute the individual mean for each group.\n",
    "\n",
    "3. **Calculate Sum of Squares (SS):**\n",
    "   - **Between-Group Sum of Squares (SSB):** Measures variability between groups.\n",
    "   - **Within-Group Sum of Squares (SSW):** Measures variability within each group.\n",
    "\n",
    "4. **Find Degrees of Freedom (df):**\n",
    "   - **Between-group df:** $ df_B = k - 1 $ (where $ k $ is the number of groups).\n",
    "   - **Within-group df:** $ df_W = N - k $ (where $ N $ is the total number of observations).\n",
    "\n",
    "5. **Calculate Mean Squares (MS):**\n",
    "   - **Between-group mean square:** $ MSB = \\frac{SSB}{df_B} $.\n",
    "   - **Within-group mean square:** $ MSW = \\frac{SSW}{df_W} $.\n",
    "\n",
    "6. **Compute the F-statistic:**\n",
    "   - $ F = \\frac{MSB}{MSW} $.\n",
    "\n",
    "7. **Compare F-statistic with Critical Value:**\n",
    "   - Look up the F-distribution table.\n",
    "   - If $ p $-value < significance level ($ \\alpha $), reject $ H₀ $, meaning at least one group mean differs.\n",
    "\n",
    "Certainly! Below is the One-Way ANOVA table in a similar format to the one in your image:\n",
    "\n",
    "**One-Way ANOVA Summary Table**\n",
    "| **Source of Variation**   | **Sum of Squares (SS)**                  | **Degrees of Freedom (d.f.)** | **Mean Square (MS)** (SS ÷ d.f.) | **F-ratio** (MS Between ÷ MS Within) |\n",
    "|--------------------------|-----------------------------------------|------------------------------|----------------------------------|--------------------------------------|\n",
    "| **Between Groups**       | $ SS_B = \\sum n_j (X̄_j - X̄̄)^2 $     | $ df_B = k - 1 $           | $ MS_B = SS_B / df_B $        | $ F = MS_B / MS_W $              |\n",
    "| **Within Groups**        | $ SS_W = \\sum (X_{ij} - X̄_j)^2 $     | $ df_W = N - k $           | $ MS_W = SS_W / df_W $        |                                      |\n",
    "| **Total**               | $ SS_T = \\sum (X_{ij} - X̄̄)^2 $       | $ df_T = N - 1 $           |                                  |                                      |\n",
    "\n",
    "### **Notes:**\n",
    "- $ SS_B $: Variability due to differences between group means.\n",
    "- $ SS_W $: Variability within each group.\n",
    "- $ SS_T $: Total variability in the dataset.\n",
    "- $ df_B $: Degrees of freedom for between-group variation.\n",
    "- $ df_W $: Degrees of freedom for within-group variation.\n",
    "- $ df_T $: Total degrees of freedom.\n",
    "- $ MS_B $ & $ MS_W $: Mean squares calculated by dividing SS by respective degrees of freedom.\n",
    "- $ F $: The test statistic used to compare group means.\n",
    "\n",
    "\n",
    "#### **Additional Notes**\n",
    "- **Assumptions of ANOVA:**\n",
    "  - The samples are **independent**.\n",
    "  - The data follows a **normal distribution** within each group.\n",
    "  - The **variance** across groups should be approximately equal (**homogeneity of variances**).\n",
    "\n",
    "Certainly! Here are some additional notes that might help you better understand One-Way ANOVA:\n",
    "\n",
    "### **Additional Notes on One-Way ANOVA**\n",
    "- **Assumptions of One-Way ANOVA**:\n",
    "  1. **Independence** – The observations within each group should be independent.\n",
    "  2. **Normality** – The data should be approximately normally distributed within each group.\n",
    "  3. **Homogeneity of Variances** – The variance across groups should be roughly equal (checked using Levene's test).\n",
    "  \n",
    "- **Interpretation of Results**:\n",
    "  - If the F-ratio is large and the corresponding **p-value** is less than the chosen significance level (typically 0.05), we reject the null hypothesis, meaning at least one group mean is significantly different.\n",
    "  - If the **p-value** is greater than 0.05, we fail to reject the null hypothesis, indicating no significant difference between group means.\n",
    "\n",
    "\n",
    "\n",
    "It's important to note that one-way ANOVA only determines if there is a significant difference\n",
    "between the group means; it does not identify which specific groups have significant\n",
    "differences. To determine which pairs of groups are significantly different, post-hoc tests, such\n",
    "as Tukey's HSD or Bonferroni, are conducted after a significant ANOVA result.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2807eeb",
   "metadata": {},
   "source": [
    "<Font color = \"Red\" size = 8 >Example - 1</font>\n",
    "\n",
    "\n",
    "**Problem Statement**\n",
    "\n",
    "A researcher wants to study the effect of different diets on weight loss. Three groups of participants follow different diet plans for 6 weeks:\n",
    "- **Group A:** Low-carb diet\n",
    "- **Group B:** Mediterranean diet\n",
    "- **Group C:** Vegan diet\n",
    "\n",
    "At the end of the study, their weight losses (in kg) are recorded as follows:\n",
    "\n",
    "| **Group A (Low-carb)** | **Group B (Mediterranean)** | **Group C (Vegan)** |\n",
    "|------------------------|----------------------------|----------------------|\n",
    "| 4.2 | 3.8 | 2.5 |\n",
    "| 3.9 | 4.1 | 3.2 |\n",
    "| 4.4 | 3.5 | 3.1 |\n",
    "| 4.1 | 4.0 | 2.8 |\n",
    "| 3.7 | 3.6 | 2.9 |\n",
    "\n",
    "**Objective:** Determine if there is a significant difference in the average weight loss among the three diet groups using **One-Way ANOVA**.\n",
    "\n",
    "\n",
    "\n",
    "**Step 1: Define Hypotheses**\n",
    "- **Null Hypothesis ($H_0$)**: There is no significant difference between the mean weight losses of the three diet plans. $ \\mu_A = \\mu_B = \\mu_C $.\n",
    "- **Alternative Hypothesis ($H_1$)**: At least one group has a significantly different mean weight loss.\n",
    "\n",
    "\n",
    "\n",
    "**Step 2: Compute Group Means and Grand Mean**\n",
    "Calculate the mean weight loss for each group:\n",
    "\n",
    "- **Mean of Group A** ($\\bar{X}_A$): $ \\frac{4.2 + 3.9 + 4.4 + 4.1 + 3.7}{5} = 4.06 $\n",
    "- **Mean of Group B** ($\\bar{X}_B$): $ \\frac{3.8 + 4.1 + 3.5 + 4.0 + 3.6}{5} = 3.8 $\n",
    "- **Mean of Group C** ($\\bar{X}_C$): $ \\frac{2.5 + 3.2 + 3.1 + 2.8 + 2.9}{5} = 2.9 $\n",
    "- **Grand Mean** ($\\bar{X}$): $ \\frac{(4.06 + 3.8 + 2.9) \\times 5}{15} = 3.59 $\n",
    "\n",
    "\n",
    "\n",
    "**Step 3: Compute Sum of Squares (SS)**\n",
    "\n",
    "**Between-Group Sum of Squares ($SS_B$)**:\n",
    "$$\n",
    "SS_B = \\sum n_j (\\bar{X}_j - \\bar{X})^2\n",
    "$$\n",
    "$$\n",
    "= 5(4.06 - 3.59)^2 + 5(3.8 - 3.59)^2 + 5(2.9 - 3.59)^2\n",
    "$$\n",
    "$$\n",
    "= 5(0.47)^2 + 5(0.21)^2 + 5(-0.69)^2\n",
    "$$\n",
    "$$\n",
    "= 5(0.2209) + 5(0.0441) + 5(0.4761)\n",
    "$$\n",
    "$$\n",
    "= 1.1045 + 0.2205 + 2.3805 = 3.7055\n",
    "$$\n",
    "\n",
    "**Within-Group Sum of Squares ($SS_W$)**:\n",
    "$$\n",
    "SS_W = \\sum (X_{ij} - \\bar{X}_j)^2\n",
    "$$\n",
    "\n",
    "Using each observation:\n",
    "\n",
    "$$\n",
    "SS_W = \\sum_{A} (X_i - \\bar{X}_A)^2 + \\sum_{B} (X_i - \\bar{X}_B)^2 + \\sum_{C} (X_i - \\bar{X}_C)^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "= (4.2 - 4.06)^2 + (3.9 - 4.06)^2 + (4.4 - 4.06)^2 + (4.1 - 4.06)^2 + (3.7 - 4.06)^2\n",
    "$$\n",
    "$$\n",
    "+ (3.8 - 3.8)^2 + (4.1 - 3.8)^2 + (3.5 - 3.8)^2 + (4.0 - 3.8)^2 + (3.6 - 3.8)^2\n",
    "$$\n",
    "$$\n",
    "+ (2.5 - 2.9)^2 + (3.2 - 2.9)^2 + (3.1 - 2.9)^2 + (2.8 - 2.9)^2 + (2.9 - 2.9)^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "= 0.023 + 0.0256 + 0.116 + 0.0016 + 0.1296 + 0 + 0.09 + 0.09 + 0.04 + 0.04 + 0.16 + 0.09 + 0.04 + 0.01 + 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "= 0.2964 + 0.26 + 0.5 = 1.0564\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "**Step 4: Compute Degrees of Freedom (df)**\n",
    "- **Between-group $ df_B = k - 1 = 3 - 1 = 2 $**\n",
    "- **Within-group $ df_W = N - k = 15 - 3 = 12 $**\n",
    "\n",
    "\n",
    "\n",
    "**Step 5: Compute Mean Squares (MS)**\n",
    "$$\n",
    "MS_B = \\frac{SS_B}{df_B} = \\frac{3.7055}{2} = 1.85275\n",
    "$$\n",
    "$$\n",
    "MS_W = \\frac{SS_W}{df_W} = \\frac{1.0564}{12} = 0.088033\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "**Step 6: Compute F-Ratio**\n",
    "$$\n",
    "F = \\frac{MS_B}{MS_W} = \\frac{1.85275}{0.088033} = 21.04\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "**Step 7: Compare F-Statistic with Critical Value**\n",
    "Using an F-table with **df (2, 12)** and **α = 0.05**, the critical value is approximately **3.89**.\n",
    "\n",
    "Since **21.04 > 3.89**, we **reject the null hypothesis** and conclude that there is a **significant difference in weight loss** among the three diets.\n",
    "\n",
    "\n",
    "\n",
    "**Step 8: Post-Hoc Analysis (If Required)**\n",
    "Since we rejected $H_0$, we may use **Tukey’s HSD** or **Bonferroni correction** to determine which groups significantly differ.\n",
    "\n",
    "\n",
    "\n",
    "### **Final Conclusion**\n",
    "Based on One-Way ANOVA, there is enough evidence to conclude that different diet plans lead to different weight loss results. A deeper analysis (post-hoc tests) can determine which diets significantly differ.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b69b639",
   "metadata": {},
   "source": [
    "<font color = \"red\"># 📘 Post Hoc Tests (After ANOVA) </font>\n",
    "\n",
    "**🔍 What Are Post Hoc Tests?**\n",
    "\n",
    "* Post hoc = *\"after this\"*. These tests are used **after running ANOVA** (Analysis of Variance).\n",
    "* If ANOVA shows a **significant difference** between group means, it tells us **that at least two groups differ**—but **not which ones**.\n",
    "* That’s where **post hoc tests** come in. They help us compare **all group pairs** to see **which specific pairs** differ significantly.\n",
    "\n",
    "\n",
    "\n",
    "**🧠 Why Can’t We Just Run Multiple t-tests?**\n",
    "\n",
    "Imagine you have 4 groups: A, B, C, and D.\n",
    "\n",
    "You want to compare:\n",
    "\n",
    "* A vs B  \n",
    "* A vs C  \n",
    "* A vs D  \n",
    "* B vs C  \n",
    "* B vs D  \n",
    "* C vs D  \n",
    "\n",
    "That’s **6 comparisons**!\n",
    "\n",
    "If you use the typical significance level of α = 0.05 for **each**, there's a high chance of **making a Type I error** (false positive) just by luck. This is called the **family-wise error rate (FWER)** problem.\n",
    "\n",
    "\n",
    "\n",
    "**🎯 Purpose of Post Hoc Tests**\n",
    "\n",
    "* **Control the Family-Wise Error Rate (FWER)**: Make sure we don’t accidentally claim there's a difference when there isn’t.\n",
    "* **Adjust significance levels** for multiple comparisons.\n",
    "* **Tell you exactly which group means are different** after an ANOVA tells you that *some difference* exists.\n",
    "\n",
    "\n",
    "**🔧 Common Post Hoc Tests**\n",
    "\n",
    "**1. 📏 Bonferroni Correction**\n",
    "\n",
    "* It’s simple and conservative.\n",
    "* Adjust the significance level by dividing it by the number of comparisons.\n",
    "\n",
    "🔍 Formula:\n",
    "\n",
    "$$\n",
    "\\alpha_{adjusted} = \\frac{\\alpha}{k}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $\\alpha$ = original significance level (e.g., 0.05)  \n",
    "* $k$ = number of pairwise comparisons\n",
    "\n",
    "**✅ Use When:**\n",
    "\n",
    "* You want a **safe**, straightforward method.  \n",
    "* You're okay with being conservative (i.e., might miss some real differences).\n",
    "\n",
    "**⚠️ Downside:**\n",
    "\n",
    "* It becomes **too strict** if you have many comparisons → **lower power** to detect true differences.\n",
    "\n",
    "🧪 Example:\n",
    "\n",
    "You have 4 groups (A, B, C, D) → 6 comparisons.\n",
    "\n",
    "If $\\alpha = 0.05$, then:\n",
    "\n",
    "$$\n",
    "\\alpha_{adjusted} = \\frac{0.05}{6} \\approx 0.0083\n",
    "$$\n",
    "\n",
    "Now, **only p-values less than 0.0083** are considered significant.\n",
    "\n",
    "\n",
    "\n",
    "2. 🍗 Tukey’s HSD (Honestly Significant Difference)\n",
    "\n",
    "* It compares all possible group pairs, controlling the **FWER** just like Bonferroni.\n",
    "* It’s **less conservative** than Bonferroni, so **more power** to detect real differences.\n",
    "* Requires:\n",
    "  * **Equal sample sizes** in each group  \n",
    "  * **Equal variances** (assumption of homogeneity of variance)\n",
    "\n",
    "**🔍 What Do We Mean by “Less Conservative”? (Bonferroni vs. Tukey’s HSD)**\n",
    "\n",
    "When we say **Tukey’s HSD is less conservative than Bonferroni**, we mean it is **less strict** in determining statistical significance. It balances between detecting **real differences** and avoiding **false positives**.\n",
    "\n",
    "\n",
    "\n",
    "**🧪 Example to Understand This:**\n",
    "\n",
    "Let’s say you're comparing **6 group pairs**:  \n",
    "(A vs B, A vs C, A vs D, B vs C, B vs D, C vs D)\n",
    "\n",
    "\n",
    "\n",
    "**📏 Bonferroni Correction:**\n",
    "\n",
    "* Adjusts significance level like this:\n",
    "  \n",
    "  $$\n",
    "  \\alpha_{adjusted} = \\frac{0.05}{6} \\approx 0.0083\n",
    "  $$\n",
    "\n",
    "* You only accept **p-values < 0.0083** as significant.\n",
    "* ✅ Very cautious → Protects against false positives (Type I error).\n",
    "* ❌ But it may **miss real differences** → Lower power.\n",
    "\n",
    "\n",
    "\n",
    "**🍗 Tukey’s HSD:**\n",
    "\n",
    "* Also adjusts for multiple comparisons.\n",
    "* Uses a **studentized range distribution** to control FWER.\n",
    "* Allows significance at **p-values closer to 0.05** (depending on sample size and variability).\n",
    "* ✅ **More power** to detect real differences.\n",
    "* ❌ Slightly higher chance of false positives than Bonferroni, but still controlled.\n",
    "\n",
    "**✅ In Summary:**\n",
    "\n",
    "| Method           | Adjusted α | Strictness | Power to Detect Real Difference |\n",
    "|------------------|------------|------------|----------------------------------|\n",
    "| Bonferroni       | 0.0083     | Very High  | Lower                            |\n",
    "| Tukey’s HSD      | ~0.03–0.05 | Moderate   | Higher                           |\n",
    "\n",
    "**Tukey’s HSD is less conservative**, meaning it is **more flexible** than Bonferroni and better at **finding true effects**, while still keeping error rates under control.\n",
    "\n",
    "\n",
    "\n",
    "**🔍 Formula (FYI):**\n",
    "\n",
    "$$\n",
    "\\text{HSD} = q \\cdot \\sqrt{\\frac{MS_{within}}{n}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $q$ = studentized range statistic (depends on number of groups and degrees of freedom)  \n",
    "* $MS_{within}$ = mean square within groups (from ANOVA)  \n",
    "* $n$ = sample size per group\n",
    "\n",
    "**✅ Use When:**\n",
    "\n",
    "* You meet assumptions (equal sample size, equal variance)\n",
    "* You want to know **which group means differ**\n",
    "\n",
    "**🧪 Example:**\n",
    "\n",
    "Let’s say you ran a one-way ANOVA with 3 groups:\n",
    "\n",
    "* Group A (mean = 50)  \n",
    "* Group B (mean = 60)  \n",
    "* Group C (mean = 55)\n",
    "\n",
    "ANOVA says: ✅ “There’s a significant difference”\n",
    "\n",
    "Now you run Tukey's HSD:\n",
    "\n",
    "* A vs B → significant (if difference > HSD threshold)  \n",
    "* A vs C → maybe not significant  \n",
    "* B vs C → maybe not significant  \n",
    "\n",
    "It tells you **exactly which groups are different** in a way that controls for errors.\n",
    "\n",
    "\n",
    "**✅ Summary Table for Revision**\n",
    "\n",
    "| Test        | Adjusts for Multiple Comparisons | Assumptions                         | Conservative? | Power                        | Use When...                              |\n",
    "| ----------- | -------------------------------- | ----------------------------------- | ------------- | ---------------------------- | ---------------------------------------- |\n",
    "| Bonferroni  | Yes                              | No strong assumptions               | Yes           | Low (many groups = stricter) | You want simplicity and safety           |\n",
    "| Tukey's HSD | Yes                              | Equal sample sizes, equal variances | Medium        | Higher                       | You meet assumptions and want more power |\n",
    "\n",
    "\n",
    "\n",
    "**🔁 Final Recap**\n",
    "\n",
    "1. **Run ANOVA** → Checks if *any* groups are different.  \n",
    "2. If significant → Use **Post Hoc Tests** to find **which** groups differ.  \n",
    "3. **Bonferroni**: Divide α by number of tests, very strict.  \n",
    "4. **Tukey’s HSD**: Good balance between error control and power, but needs equal sample sizes and variance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29cbd05",
   "metadata": {},
   "source": [
    "<font color = \"Red\" size = 27>**Two - Way ANOVA**</font>\n",
    "\n",
    "Coming Soon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480c2bb7",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
