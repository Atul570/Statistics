{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e213192",
   "metadata": {},
   "source": [
    "<font color = \"red\" size = 12>üìò Linear Regression: Step-by-Step Derivation Notes</font>\n",
    "\n",
    "---\n",
    "\n",
    "**‚úçÔ∏è 1. Objective of Linear Regression**\n",
    "\n",
    "We want to find a line:\n",
    "\n",
    "$$\n",
    "y = mx + b\n",
    "$$\n",
    "\n",
    "That **best fits** the given data points $ (x_1, y_1), (x_2, y_2), \\dots, (x_n, y_n) $.\n",
    "Our goal is to minimize the **error between the actual values and predicted values**.\n",
    "\n",
    "\n",
    "\n",
    "**üìâ 2. Error or Loss Function (Sum of Squared Errors)**\n",
    "\n",
    "We define the **error (residual)** for each data point as:\n",
    "\n",
    "$$\n",
    "e_i = y_i - \\hat{y}_i = y_i - (mx_i + b)\n",
    "$$\n",
    "\n",
    "- $\\hat{y}$ = Is Predicted Value\n",
    "\n",
    "To penalize large errors and avoid cancellation of positive/negative errors, we square them:\n",
    "\n",
    "$$\n",
    "E = \\sum_{i=1}^{n} (y_i - (mx_i + b))^2\n",
    "$$\n",
    "\n",
    "This is called the **Loss Function** or **Cost Function**:\n",
    "\n",
    "$$\n",
    "J(m, b) = \\sum_{i=1}^{n} (y_i - mx_i - b)^2\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "**üîç 3. Goal: Minimize the Loss Function**\n",
    "\n",
    "We want to find the values of $ m $ and $ b $ such that $ J(m, b) $ is **minimum**.\n",
    "\n",
    "To do this, we apply **differentiation** (calculus) and set partial derivatives to zero.\n",
    "\n",
    "\n",
    "\n",
    "**üßÆ 4. Differentiation of Loss Function**\n",
    "\n",
    "Expanding:\n",
    "\n",
    "$$\n",
    "J(m, b) = \\sum_{i=1}^{n} (y_i - mx_i - b)^2\n",
    "$$\n",
    "\n",
    "We take **partial derivative** of \\( J \\) w.r.t. \\( m \\) and \\( b \\).\n",
    "\n",
    "\n",
    "\n",
    "**üîπ Partial Derivative w.r.t. \\( m \\):**\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial m} = \\sum_{i=1}^{n} 2(y_i - mx_i - b)(-x_i)\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "**üîπ Partial Derivative w.r.t. \\( b \\):**\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial b} = \\sum_{i=1}^{n} 2(y_i - mx_i - b)(-1)\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "Set both partial derivatives to zero for minimization:\n",
    "\n",
    "\n",
    "\n",
    "**üß© 5. Solving for $ m $ and $ b $ (Closed-form)**\n",
    "\n",
    "Solving step-by-step:\n",
    "\n",
    "**Step 1: Solve for \\( b \\)**\n",
    "\n",
    "From $ \\frac{\\partial J}{\\partial b} = 0 $:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{n} (y_i - mx_i - b) = 0\n",
    "$$\n",
    "\n",
    "Distribute sum:\n",
    "\n",
    "$$\n",
    "\\sum y_i - m\\sum x_i - nb = 0\n",
    "$$\n",
    "\n",
    "Rearrange:\n",
    "\n",
    "$$\n",
    "b = \\frac{\\sum y_i - m\\sum x_i}{n}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "**Step 2: Solve for $ m $**\n",
    "\n",
    "From $ \\frac{\\partial J}{\\partial m} = 0 $:\n",
    "\n",
    "$$\n",
    "\\sum (y_i - mx_i - b)x_i = 0\n",
    "$$\n",
    "\n",
    "Expanding:\n",
    "\n",
    "$$\n",
    "\\sum y_i x_i - m\\sum x_i^2 - b\\sum x_i = 0\n",
    "$$\n",
    "\n",
    "Solving simultaneously, we get:\n",
    "\n",
    "\n",
    "\n",
    "**‚úÖ Final Formula for $ m $ (slope):**\n",
    "\n",
    "$$\n",
    "m = \\frac{\\sum (x_i - \\bar{x}) (y_i - \\bar{y})}{\\sum (x_i - \\bar{x})^2}\n",
    "$$\n",
    "\n",
    "or equivalently,\n",
    "\n",
    "$$\n",
    "m = \\frac{n\\sum x_i y_i - \\sum x_i \\sum y_i}{n\\sum x_i^2 - (\\sum x_i)^2}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "**‚úÖ Final Formula for \\( b \\) (intercept):**\n",
    "\n",
    "$$\n",
    "b = \\bar{y} - m\\bar{x}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "$$\n",
    "\\bar{x} = \\frac{1}{n} \\sum x_i, \\quad \\bar{y} = \\frac{1}{n} \\sum y_i\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "# üìä 6. Example Calculation\n",
    "\n",
    "Let's use the dataset:\n",
    "\n",
    "| $ x $ | $ y $ |\n",
    "| --- | --- |\n",
    "| 1 | 2 |\n",
    "| 2 | 2.5 |\n",
    "| 3 | 3 |\n",
    "| 4 | 4.5 |\n",
    "| 5 | 5 |\n",
    "\n",
    "**Step 1: Compute Summations**\n",
    "\n",
    "**Compute Means:**\n",
    "$$\n",
    "\\bar{x} = \\frac{1+2+3+4+5}{5} = 3\n",
    "$$\n",
    "$$\n",
    "\\bar{y} = \\frac{2+2.5+3+4.5+5}{5} = 3.4\n",
    "$$\n",
    "\n",
    "**Compute $ \\sum x_i y_i $, $ \\sum x_i^2 $:**\n",
    "\n",
    "$$\n",
    "\\sum x_i y_i = (1)(2) + (2)(2.5) + (3)(3) + (4)(4.5) + (5)(5) = 42\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sum x_i^2 = 1^2 + 2^2 + 3^2 + 4^2 + 5^2 = 55\n",
    "$$\n",
    "\n",
    "**Step 2: Compute $ m $ and $ b $**\n",
    "\n",
    "Using the formulas:\n",
    "\n",
    "$$\n",
    "m = \\frac{5(42) - (1+2+3+4+5)(2+2.5+3+4.5+5)}{5(55) - (1+2+3+4+5)^2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "m = \\frac{210 - 85}{275 - 225} = \\frac{125}{50} = 2.5\n",
    "$$\n",
    "\n",
    "$$\n",
    "b = \\bar{y} - m\\bar{x} = 3.4 - (2.5)(3) = 3.4 - 7.5 = -4.1\n",
    "$$\n",
    "\n",
    "**‚úÖ Final Regression Line:**\n",
    "\n",
    "$$\n",
    "y = 2.5x - 4.1\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "**üöÄ 7. Bonus (Optional for Future - Matrix Form)**\n",
    "\n",
    "In vectorized/matrix form, the closed-form solution is:\n",
    "\n",
    "$$\n",
    "\\theta = (X^TX)^{-1}X^Ty\n",
    "$$\n",
    "\n",
    "Where $ X $ is the design matrix including the bias term (column of 1s), and $ y $ is the target vector.\n",
    "\n",
    "\n",
    "\n",
    "**üìö Prerequisites**\n",
    "\n",
    "To fully understand this:\n",
    "\n",
    "* Basic algebra\n",
    "* Summation rules\n",
    "* Differentiation (partial derivatives)\n",
    "* Concept of minimization\n",
    "* Basic statistics: mean ($\\bar{x}, \\bar{y}$)\n",
    "\n",
    "\n",
    "\n",
    "**‚úÖ Summary Box (Quick Reference)**\n",
    "\n",
    "| Element | Formula |\n",
    "| --- | --- |\n",
    "| Error | $ e_i = y_i - (mx_i + b) $ |\n",
    "| Loss Function | $ J(m, b) = \\sum (y_i - mx_i - b)^2 $ |\n",
    "| Slope (m) | $ m = \\frac{n\\sum xy - \\sum x \\sum y}{n\\sum x^2 - (\\sum x)^2} $ |\n",
    "| Intercept (b) | $ b = \\bar{y} - m\\bar{x} $ |\n",
    "| Final Line | $ \\hat{y} = mx + b $ |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b238e7",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
