{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b756f007",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e472d9b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "ðŸ“˜ **Ridge Regression - Notes**\n",
    "\n",
    "\n",
    "ðŸ”¹ **What is Ridge Regression?**\n",
    "\n",
    "Ridge Regression is a **regularized version of Linear Regression** that adds a penalty term to reduce model complexity and prevent **overfitting**.\n",
    "\n",
    "> ðŸš¨ It helps when we have **multicollinearity** (features are highly correlated) or when the number of features is **close to or greater than** the number of observations.\n",
    "\n",
    "\n",
    "\n",
    "ðŸ”¹ **Why do we need it?**\n",
    "\n",
    "In ordinary linear regression:\n",
    "\n",
    "$$\n",
    "y = w_0 + w_1 x_1 + w_2 x_2 + \\dots + w_n x_n\n",
    "$$\n",
    "\n",
    "We minimize the **Mean Squared Error (MSE)**:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(w) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - w^T x_i)^2\n",
    "$$\n",
    "Of course, ATUL! Letâ€™s break it down carefully.\n",
    "\n",
    "In your formula:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(w) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - w^T x_i)^2\n",
    "$$\n",
    "\n",
    "Hereâ€™s what each part means:\n",
    "\n",
    "* $y_i$: actual target value for the $i$-th data point.\n",
    "* $\\hat{y}_i$: predicted value for the $i$-th data point.\n",
    "* $x_i$: the feature vector (a column vector) for the $i$-th data point.\n",
    "* $w$: the weight vector (also a column vector) that our model is trying to learn.\n",
    "\n",
    "Now, **$w^T x_i$** means the transpose of $w$ multiplied by $x_i$:\n",
    "\n",
    "* $w^T$: transpose of the weight vector $w$.\n",
    "* When you do $w^T x_i$, you get a scalar (single number), which becomes the prediction $\\hat{y}_i$.\n",
    "\n",
    "### Why transpose?\n",
    "\n",
    "If:\n",
    "\n",
    "* $w \\in \\mathbb{R}^d$ (a column vector with $d$ elements).\n",
    "* $x_i \\in \\mathbb{R}^d$ (also a column vector with $d$ elements).\n",
    "\n",
    "Then:\n",
    "\n",
    "* $w^T \\in \\mathbb{R}^{1 \\times d}$ is a row vector.\n",
    "* $w^T x_i$ is then a dot product, resulting in a scalar.\n",
    "\n",
    "So, **$w^T x_i$** is the predicted value for the $i$-th observation in a linear regression model.\n",
    "\n",
    "\n",
    "\n",
    "But if:\n",
    "\n",
    "* There are **too many features**\n",
    "* Or features are **highly correlated**\n",
    "\n",
    "â†’ The model becomes **unstable**, and coefficients may become large.\n",
    "\n",
    "\n",
    "\n",
    "ðŸ”¹ **Ridge Regression Loss Function**\n",
    "\n",
    "Ridge adds a **penalty** (regularization term):\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{ridge}}(w) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - w^T x_i)^2 + \\lambda \\sum_{j=1}^{p} w_j^2\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $\\lambda$ = Regularization parameter (controls penalty strength)\n",
    "* $\\sum w_j^2$ = L2 norm (squared magnitude of weights)\n",
    "* Note: Bias term $w_0$ is often not penalized.\n",
    "\n",
    "\n",
    "\n",
    "ðŸ”¹ **Key Intuition**\n",
    "\n",
    "* Ridge **shrinks the weights**, but **doesnâ€™t make them exactly zero**.\n",
    "* Controls overfitting by adding **bias** to reduce **variance**.\n",
    "\n",
    "\n",
    "\n",
    "ðŸ”¹ **Derivative of Ridge Loss Function** (Univariate Case for Simplicity)\n",
    "\n",
    "Letâ€™s take a simple case:\n",
    "\n",
    "$$\n",
    "y_i = w x_i\n",
    "$$\n",
    "\n",
    "Loss function:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(w) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - w x_i)^2 + \\lambda w^2\n",
    "$$\n",
    "\n",
    "Now derive the gradient w\\.r.t. $w$:\n",
    "\n",
    "$$\n",
    "\\frac{d}{dw} \\left[ \\frac{1}{n} \\sum (y_i - w x_i)^2 + \\lambda w^2 \\right]\n",
    "= \\frac{1}{n} \\sum -2 x_i (y_i - w x_i) + 2\\lambda w\n",
    "$$\n",
    "\n",
    "$$\n",
    "= -\\frac{2}{n} \\sum x_i (y_i - w x_i) + 2\\lambda w\n",
    "$$\n",
    "\n",
    "Set gradient to zero for optimal $w$:\n",
    "\n",
    "$$\n",
    "-\\frac{2}{n} \\sum x_i (y_i - w x_i) + 2\\lambda w = 0\n",
    "$$\n",
    "\n",
    "This leads to the regularized normal equation in general form.\n",
    "\n",
    "\n",
    "\n",
    "ðŸ”¹ **Closed-Form Solution (Matrix Form)**\n",
    "\n",
    "For multivariate case with:\n",
    "\n",
    "* $X$ = feature matrix (n x p)\n",
    "* $y$ = target vector\n",
    "* $w$ = weight vector\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "w = (X^T X + \\lambda I)^{-1} X^T y\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $I$ is the identity matrix\n",
    "* $\\lambda$ prevents the matrix $X^T X$ from being singular\n",
    "\n",
    "\n",
    "\n",
    "ðŸ”¹ **Effect of Lambda (Î»):**\n",
    "\n",
    "* $\\lambda = 0$ â†’ same as Linear Regression\n",
    "* $\\lambda \\to \\infty$ â†’ coefficients shrink toward zero\n",
    "* Choosing $\\lambda$ is critical â†’ use **Cross-Validation**\n",
    "\n",
    "\n",
    "\n",
    "ðŸ”¹ **Pros and Cons**\n",
    "\n",
    "âœ… Pros:\n",
    "\n",
    "* Handles multicollinearity well\n",
    "* Improves model generalization\n",
    "* Prevents overfitting\n",
    "\n",
    "âŒ Cons:\n",
    "\n",
    "* Doesn't perform feature selection (unlike Lasso)\n",
    "* All coefficients are reduced but **none become zero**\n",
    "\n",
    "\n",
    "ðŸ”¹ **Ridge vs. Linear vs. Lasso**\n",
    "\n",
    "| Property            | Linear Regression | Ridge Regression | Lasso Regression |\n",
    "| ------------------- | ----------------- | ---------------- | ---------------- |\n",
    "| Regularization      | âŒ No              | âœ… L2             | âœ… L1             |\n",
    "| Coefficients Shrink | âŒ No              | âœ… Yes            | âœ… Some = 0       |\n",
    "| Feature Selection   | âŒ No              | âŒ No             | âœ… Yes            |\n",
    "\n",
    "\n",
    "\n",
    "ðŸ“Œ Summary:\n",
    "\n",
    "> Ridge Regression = **Linear Regression + L2 penalty**\n",
    "\n",
    "\n",
    "<font color = \"Red\">**Continue With Lecture 55 From !00 days of Machine Learning (** Lasso and Elastic Left**)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed7cc62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7e3ff693",
   "metadata": {},
   "source": [
    "ss"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
